{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMl5TDHVdHXyz1IxJu7t4bn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MehraeenTimas/nlp-course/blob/main/NLTK_spaCy_setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JoZSuOZfLI6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB_U7m8FJk5Q"
      },
      "source": [
        "## 1. NLP Toolkit\n",
        "1. **[NLTK (Natural Language Toolkit)](https://en.wikipedia.org/wiki/Natural_Language_Toolkit)** is a comprehensive open-source Python library designed for *educational* and *research* purposes in natural language processing. It provides robust tools for tasks like tokenization, stemming, lemmatization, parsing, and more, backed by extensive corpora and lexical resources. [The Natural Language Toolkit](https://www.nltk.org/)\n",
        "\n",
        "2. **[spaCy](https://en.wikipedia.org/wiki/SpaCy)** is an *industrial-strength* NLP library in Python tailored for real-world applications, emphasizing speed and accuracy in tasks such as tokenization, named entity recognition, and dependency parsing. Its modern API and efficient design make it ideal for processing large-scale text data. [spaCy Documentation](https://spacy.io/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIYdn1woOS1n"
      },
      "outputs": [],
      "source": [
        "# --- NLTK Setup ---\n",
        "!pip install nltk\n",
        "import nltk\n",
        "# Download essential NLTK datasets: tokenizers, stopwords, WordNet, etc.\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# --- spaCy Setup ---\n",
        "!pip install spacy\n",
        "import spacy\n",
        "\n",
        "# Download and load the small English model for spaCy\n",
        "!python -m spacy download en_core_web_sm\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubVO_GdERyVL"
      },
      "source": [
        "### **WordNet:**\n",
        "WordNet is a comprehensive lexical database within NLTK that organizes English words into synonym sets (synsets) to facilitate semantic analysis; alternatives like BabelNet or ConceptNet extend these capabilities to multilingual and broader semantic relationships.\n",
        "\n",
        "### **Gutenberg:**\n",
        "The Gutenberg corpus in NLTK is a curated collection of classic literary texts sourced from Project Gutenberg, offering diverse works for exploring historical language patterns and stylistic nuances; alternatives like the Brown Corpus or Reuters Corpus provide additional perspectives for varied text analysis.\n",
        "\n",
        "### **Punkt:**\n",
        "Punkt is an unsupervised sentence tokenizer in NLTK that intelligently segments text into sentences using learned punctuation patterns, while spaCy’s built-in sentence segmentation offers a statistical alternative with robust boundary detection.\n",
        "\n",
        "### **en_core_web_sm:**\n",
        "The spaCy model en_core_web_sm is a lightweight yet efficient English model for tasks such as tokenization, POS tagging, dependency parsing, and NER; for enhanced accuracy and richer features, consider its larger counterparts—en_core_web_md, en_core_web_lg, or the transformer-based en_core_web_trf."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fbQpjeXaxLo"
      },
      "outputs": [],
      "source": [
        "# our example in this work\n",
        "short_text = \"Natural Language Processing is fun and educational.\"\n",
        "long_text = 'Dr. Smith, traveled to Washington, D.C. on Jan. 5th for a cutting-edge NLP conference. During his keynote, he explained that advancements in tokenization techniques—particularly those implemented in NLTK and spaCy (e.g., handling abbreviations like \"Dr.\" and \"e.g.\" seamlessly)—are transforming text analysis.'\n",
        "text_emails = \"Contact us at admin.support_34@example.com or sales-dep@company.org for inquiries.\"\n"
      ]
    }
  ]
}