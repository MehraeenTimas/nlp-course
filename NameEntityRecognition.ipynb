{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNX9YjYVoE8fMigEFerYhR7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MehraeenTimas/nlp-course/blob/main/NameEntityRecognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HT4B0xcaQuee"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qckyl7jxaczJ"
      },
      "source": [
        "## Name Entity Recognition (NER)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAwpv_3Xagpm",
        "outputId": "36705188-a5ef-4863-a81f-8c8ba686c521"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Named Entities:\n",
            "Apple Inc. -> ORG\n",
            "OpenAI -> GPE\n",
            "Oscar Award -> WORK_OF_ART\n",
            "California -> GPE\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"Apple Inc. announced a new partnership with OpenAI at the annual Oscar Award event in California.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"Named Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text} -> {ent.label_}\")\n",
        "\n",
        "\n",
        "# ORG (Organization)\n",
        "# GPE (Geopolitical Entity)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7SooLNC6Gpn",
        "outputId": "effdd7a6-1e8e-45ed-f101-c17d5554ad16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text (First 500 characters):\n",
            " [the tragedie of hamlet by william shakespeare 1599]   actus primus. scoena prima.  enter barnardo and francisco two centinels.    barnardo. who's there?   fran. nay answer me: stand & vnfold your selfe     bar. long liue the king     fran. barnardo?   bar. he     fran. you come most carefully vpon your houre     bar. 'tis now strook twelue, get thee to bed francisco     fran. for this releefe much thankes: 'tis bitter cold, and i am sicke at heart     barn. haue you had quiet guard?   fran. not\n",
            "\n",
            "Stemmed Text (First 500 characters):\n",
            " tragedi hamlet william shakespear 1599 actu primu scoena prima enter barnardo francisco two centinel barnardo fran nay answer stand vnfold self bar long liue king fran barnardo bar fran come care vpon hour bar strook twelu get thee bed francisco fran releef much thank bitter cold sick heart barn haue quiet guard fran mous stir barn well goodnight meet horatio marcellu riual watch bid make hast enter horatio marcellu fran think hear stand hor friend ground mar dane fran giue good night mar farwel\n",
            "\n",
            "Lemmatized Text (First 500 characters):\n",
            " tragedie hamlet william shakespeare 1599 actus primus scoena prima enter barnardo francisco two centinels barnardo fran nay answer stand vnfold selfe bar long liue king fran barnardo bar fran come carefully vpon houre bar strook twelue get thee bed francisco fran releefe much thankes bitter cold sicke heart barn haue quiet guard fran mouse stirring barn well goodnight meet horatio marcellus riuals watch bid make hast enter horatio marcellus fran thinke heare stand hor friend ground mar dane fran\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import gutenberg, stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "\n",
        "# Select a text file from Gutenberg (e.g., 'shakespeare-hamlet.txt')\n",
        "file_id = \"shakespeare-hamlet.txt\"\n",
        "raw_text = gutenberg.raw(file_id)\n",
        "\n",
        "# Step 1: Text Cleaning (Removing Gutenberg Header/Footer)\n",
        "def clean_text(text):\n",
        "    lines = text.split(\"\\n\") # break (enter - new line)\n",
        "    start_idx, end_idx = 0, len(lines)\n",
        "\n",
        "    # Removing Gutenberg boilerplate (First few and last few lines)\n",
        "    for i, line in enumerate(lines):\n",
        "        if \"START OF THIS PROJECT GUTENBERG\" in line:\n",
        "            start_idx = i + 1\n",
        "        if \"END OF THIS PROJECT GUTENBERG\" in line:\n",
        "            end_idx = i\n",
        "            break\n",
        "\n",
        "    cleaned_lines = lines[start_idx:end_idx]\n",
        "    cleaned_text = \" \".join(cleaned_lines)\n",
        "    return cleaned_text\n",
        "\n",
        "text = clean_text(raw_text)\n",
        "\n",
        "# Step 2: Lowercase\n",
        "text = text.lower()\n",
        "\n",
        "# Step 3: Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Step 4: Remove Punctuation & Stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "\n",
        "# Step 5: Stemming & Lemmatization\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "# Step 6: Convert back to text\n",
        "stemmed_text = \" \".join(stemmed_tokens)\n",
        "lemmatized_text = \" \".join(lemmatized_tokens)\n",
        "\n",
        "# Output Results\n",
        "print(\"Original Text (First 500 characters):\\n\", text[:500])\n",
        "print(\"\\nStemmed Text (First 500 characters):\\n\", stemmed_text[:500])\n",
        "print(\"\\nLemmatized Text (First 500 characters):\\n\", lemmatized_text[:500])\n"
      ]
    }
  ]
}