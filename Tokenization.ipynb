{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOj/Yy/fEzKiaAvdBb/tRHY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MehraeenTimas/nlp-course/blob/main/Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ky1JaK1DMgYK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwzdlsxxK2RJ"
      },
      "source": [
        "## 3. Tokenization Techniques\n",
        "Tokenization is the process of splitting text into smaller units such as words or sentences, which simplifies subsequent text analysis and processing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xX3QUxznK5or",
        "outputId": "7a6f043d-7517-4af2-a008-64d555059b8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word Tokens: ['Dr.', 'Smith', ',', 'traveled', 'to', 'Washington', ',', 'D.C.', 'on', 'Jan.', '5th', 'for', 'a', 'cutting-edge', 'NLP', 'conference', ',', 'During', 'his', 'keynote', ',', 'he', 'explained', 'that', 'advancements', 'in', 'tokenization', 'techniques—particularly', 'those', 'implemented', 'in', 'NLTK', 'and', 'spaCy', '(', 'e.g.', ',', 'handling', 'abbreviations', 'like', '``', 'Dr.', \"''\", 'and', '``', 'e.g', '.', \"''\", 'seamlessly', ')', '—are', 'transforming', 'text', 'analysis', '.']\n",
            "Sentence Tokens: ['Dr. Smith, traveled to Washington, D.C. on Jan. 5th for a cutting-edge NLP conference, During his keynote, he explained that advancements in tokenization techniques—particularly those implemented in NLTK and spaCy (e.g., handling abbreviations like \"Dr.\" and \"e.g.\"', 'seamlessly)—are transforming text analysis.']\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "# NLTK Tokenization\n",
        "# Word Tokenization with NLTK\n",
        "long_text = 'Dr. Smith, traveled to Washington, D.C. on Jan. 5th for a cutting-edge NLP conference! During his keynote, he explained that advancements in tokenization techniques—particularly those implemented in NLTK and spaCy (e.g., handling abbreviations like \"Dr.\" and \"e.g.\" seamlessly)—are transforming text analysis.'\n",
        "\n",
        "words = nltk.word_tokenize(long_text)\n",
        "print(\"Word Tokens:\", words)\n",
        "\n",
        "# Sentence Tokenization with NLTK:\n",
        "sentences = nltk.sent_tokenize(long_text)\n",
        "print(\"Sentence Tokens:\", sentences)\n",
        "print(len(sentences))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WvUEeiKTZHc",
        "outputId": "88e69557-2858-40b5-f193-ea432dde55bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word Tokens: ['Natural', 'Language', 'Processing', 'is', 'fun', 'and', 'educational', '.']\n",
            "Sentence Tokens: ['Natural Language Processing is fun and educational.']\n"
          ]
        }
      ],
      "source": [
        "# spaCy Tokenization\n",
        "# Word Tokenization with spaCy:\n",
        "\n",
        "doc = nlp(short_text)\n",
        "words = [token.text for token in doc]\n",
        "print(\"Word Tokens:\", words)\n",
        "\n",
        "# Sentence Tokenization with spaCy\n",
        "sentences = [sent.text for sent in doc.sents]\n",
        "print(\"Sentence Tokens:\", sentences)\n"
      ]
    }
  ]
}